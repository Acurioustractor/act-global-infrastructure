# ACT Voice v0.1 - Deployment Guide
**Version:** v0.1 (Beta / Internal Testing)
**Dataset:** 90 examples, 88/100 quality score
**Status:** Ready for fine-tuning
**Date:** 2026-01-01

---

## üìä v0.1 Dataset Summary

### Quality Metrics
- **Overall Score:** 88/100 (Good - Ready for fine-tuning) ‚úÖ
- **Total Examples:** 90
- **Unique Topics:** 84
- **Estimated Training Cost:** $0.23 (one-time)
- **Status:** Exceeds production-ready target (80/100)

### LCAA Coverage
- **Listen:** 32 examples (35.6%) ‚úÖ
- **Curiosity:** 24 examples (26.7%) ‚úÖ
- **Action:** 48 examples (53.3%) ‚úÖ
- **Art:** 14 examples (15.6%) ‚ö†Ô∏è (will improve in v1.0)

### Strategic Pillars
- ‚úÖ Ethical Storytelling: 2.2%
- ‚úÖ Justice Reimagined: 1.1%
- ‚úÖ Community Resilience: 1.1%
- ‚úÖ Circular Economy: 2.2%
- ‚úÖ Regeneration at Scale: 1.1%
- ‚ö†Ô∏è Art of Social Impact: 0% (will add in v1.0)

### Voice Consistency
- Community-centered: 95.6% ‚úÖ
- LCAA language: 84.4% ‚úÖ
- Honest about challenges: 61.1% ‚úÖ
- Anti-jargon: 96.7% ‚úÖ
- Regenerative metaphors: 27.8% ‚ö†Ô∏è (will improve in v1.0)

---

## üöÄ Quickstart Deployment

### Option 1: Using Existing Script (Recommended)

```bash
# 1. Set your OpenAI API key
export OPENAI_API_KEY="sk-your-key-here"

# 2. Run deployment script
npm run knowledge:deploy-model

# The script will:
# - Upload training-data/act-voice-training-dataset-v2-2026-01-01.jsonl
# - Create fine-tuning job with gpt-4o-mini
# - Monitor training progress (10-30 minutes)
# - Save model info when complete
```

### Option 2: Manual OpenAI CLI

```bash
# 1. Install OpenAI CLI
npm install -g openai

# 2. Set API key
export OPENAI_API_KEY="sk-your-key-here"

# 3. Upload training file
openai api fine_tunes.create \
  -t training-data/act-voice-training-dataset-v2-2026-01-01.jsonl \
  -m gpt-4o-mini-2024-07-18 \
  --suffix "act-voice-v0.1" \
  --n_epochs 3

# 4. Monitor progress
openai api fine_tunes.follow -i ft-xxxxx

# 5. Use model when ready
# Model ID: gpt-4o-mini-2024-07-18:act-voice-v0.1
```

### Option 3: OpenAI Dashboard (UI)

1. Go to https://platform.openai.com/finetune
2. Click "Create fine-tuning job"
3. Upload: `training-data/act-voice-training-dataset-v2-2026-01-01.jsonl`
4. Model: `gpt-4o-mini-2024-07-18`
5. Epochs: `3`
6. Suffix: `act-voice-v0.1`
7. Click "Create" and monitor in dashboard

---

## üß™ Testing v0.1

### Test Queries (Compare vs. Base Model)

Test these questions with both base GPT-4o-mini and your fine-tuned v0.1:

#### 1. LCAA Methodology
```
Q: What does Listen mean in ACT's LCAA methodology?

Expected: Detailed explanation of Listen as deep, patient listening to place, people, history, and community voice. Should mention listening to Country, Traditional Owners, Community, Elders, Youth, and the Silenced.

Base model: Generic answer about listening
Fine-tuned: ACT-specific Listen methodology
```

#### 2. Project-Specific Knowledge
```
Q: How did Curiosity shape the Empathy Ledger platform?

Expected: Specific questions explored (What if stories had consent like FPIC?), prototyping experiments, what was learned, pivots made.

Base model: Generic or invented answer
Fine-tuned: Specific ACT Empathy Ledger Curiosity journey
```

#### 3. Strategic Pillars
```
Q: How does ACT's Circular Economy work connect to LCAA?

Expected: Goods on Country example showing Listen (beds as need) ‚Üí Curiosity (can waste become wealth?) ‚Üí Action (manufacturing) ‚Üí Art (beds as transformation)

Base model: Generic circular economy concepts
Fine-tuned: ACT-specific Goods on Country + LCAA integration
```

#### 4. Voice & Tone
```
Q: Tell me about ACT's approach to community partnerships

Expected:
- Community-centered language
- References to power transfer, not control
- Mentions 40% profit-sharing, designed obsolescence
- Humble tone ("we're learning as we go")
- LCAA methodology application

Base model: Generic partnership advice
Fine-tuned: ACT voice, values, and specific practices
```

#### 5. Cultural Protocols
```
Q: How does ACT listen to Elders in the Listen phase?

Expected: Cultural protocol details, Elder review workflows, consent processes, payment/reciprocity, relationship over transaction

Base model: Generic respect for elders
Fine-tuned: Specific ACT practices and protocols
```

---

## üìä A/B Testing Framework

### Metrics to Track

| Metric | Base Model | v0.1 Fine-Tuned | v1.0 Target |
|--------|-----------|-----------------|-------------|
| **Accuracy** (factual correctness) | Baseline | Measure | +30% |
| **Voice Match** (sounds like ACT) | Baseline | Measure | +50% |
| **LCAA Application** (uses methodology) | Baseline | Measure | +80% |
| **Project Knowledge** (specific details) | Baseline | Measure | +90% |
| **Cultural Sensitivity** (protocols respected) | Baseline | Measure | +70% |

### Testing Process

1. **Blind Testing:**
   - Give evaluators responses from both models (unlabeled)
   - Ask: "Which sounds more like ACT?"
   - Track preference rates

2. **Accuracy Scoring:**
   - 20 factual questions about ACT projects
   - Score: Correct / Partially Correct / Incorrect
   - Compare base vs. fine-tuned

3. **Voice Evaluation:**
   - Rate each response on ACT voice characteristics:
     - Community-centered (1-5)
     - Uses LCAA language (1-5)
     - Regenerative metaphors (1-5)
     - Humble/honest tone (1-5)
     - Anti-jargon (1-5)

4. **User Feedback:**
   - Internal team uses both models
   - Gather qualitative feedback
   - Identify gaps for v1.0

---

## üéØ v0.1 Goals & Expectations

### What v0.1 Should Do Well ‚úÖ
- Explain LCAA methodology (Listen, Curiosity especially strong)
- Describe major projects (Empathy Ledger, JusticeHub, Goods on Country, BCV, The Harvest)
- Use ACT voice and tone (community-centered, humble)
- Apply strategic pillars to real examples
- Respect cultural protocols and sovereignty principles

### What v0.1 May Struggle With ‚ö†Ô∏è
- Art phase details (only 14 examples - 28% of target)
- Art of Social Impact pillar (not covered in training)
- Regenerative metaphors (only 27.8% coverage)
- Very specific Art installation details
- Recent updates not in training data (Dec 2024 cutoff)

### Known Limitations
- Training data from Dec 2024 (may not reflect 2026 updates)
- Art phase under-represented
- No real user queries yet (all synthetic examples)
- May overfit to training examples

---

## üìà Success Criteria for v0.1

Deploy v0.1 as successful if it achieves:

- ‚úÖ **70%+ voice match** (sounds like ACT in blind tests)
- ‚úÖ **80%+ factual accuracy** (correct project details)
- ‚úÖ **60%+ LCAA application** (uses methodology appropriately)
- ‚úÖ **No cultural protocol violations** (zero incidents)
- ‚úÖ **Positive internal feedback** (team prefers vs. base model)

If v0.1 meets these criteria ‚Üí Proceed to v1.0 with improvements

If v0.1 underperforms ‚Üí Analyze gaps, add training data, retrain

---

## üîÑ Iteration Plan: v0.1 ‚Üí v1.0

Based on v0.1 testing, v1.0 will add:

### 1. Art Phase Expansion (+30 examples)
- The Gold.Phone, The Confessional, The Treacher installations
- Art as methodology (not decoration)
- Creative systems change examples
- Community co-creation processes

### 2. Voice Enhancement (all examples)
- Add regenerative metaphors to 80%+ of examples
- Farming/nature language: seeds, soil, cultivation, harvest, seasons
- Traditional Owner / Country references increased

### 3. Art of Social Impact Pillar (+2-3 examples)
- Complete all 6 strategic pillars
- 100% pillar coverage

### 4. Real User Query Integration
- Add 10-20 examples from actual v0.1 usage
- Address common questions not well-answered
- Fix hallucinations or errors discovered

### Expected v1.0 Outcome:
- **120 examples** (60% toward 200 target)
- **92/100 quality score**
- **Complete LCAA balance**
- **100% pillar coverage**
- **80%+ voice metrics**

---

## üíæ Model Information (Post-Deployment)

After deployment, save this information:

```json
{
  "version": "v0.1",
  "modelId": "gpt-4o-mini-2024-07-18:act-voice-v0.1",
  "baseModel": "gpt-4o-mini-2024-07-18",
  "trainingExamples": 90,
  "qualityScore": 88,
  "deployedAt": "2026-01-01",
  "purpose": "Internal testing and feedback gathering",
  "testingPeriod": "2 weeks",
  "nextVersion": "v1.0 (target: +30 examples, Art focus)"
}
```

Save to: `training-data/act-voice-model-v0.1-info.json`

---

## üÜò Troubleshooting

### Fine-tuning Job Fails
- **Error: Invalid format**
  - Check JSONL format: each line must be valid JSON
  - Validate: `node scripts/analyze-training-dataset.mjs training-data/act-voice-training-dataset-v2-2026-01-01.jsonl`

- **Error: Too few examples**
  - OpenAI requires minimum 10 examples
  - v0.1 has 90 ‚úÖ

- **Error: Token limit exceeded**
  - Check max tokens per example
  - v0.1 max: 559 tokens (well below 4096 limit) ‚úÖ

### Model Doesn't Improve Over Base
- **Possible causes:**
  - Training data not diverse enough
  - Not enough examples for specific topics
  - Overfitting to training examples

- **Solutions:**
  - Test with queries not in training data
  - Add more diverse examples in v1.0
  - Check if epochs should be reduced (currently 3)

### Model Hallucinates or Invents
- **Possible causes:**
  - Trying to answer questions outside training
  - Filling gaps with base model knowledge

- **Solutions:**
  - Add system message: "Only answer based on ACT knowledge. Say 'I don't have that specific information' if unsure."
  - Add examples of saying "I don't know"
  - Test with out-of-domain questions

---

## üìû Support & Feedback

### Internal Testing Feedback
Please provide feedback on:
1. **Voice match:** Does it sound like ACT?
2. **Accuracy:** Are project details correct?
3. **Usefulness:** Does it help you explain ACT work?
4. **Gaps:** What questions does it answer poorly?
5. **Surprises:** What did it do better/worse than expected?

### Contact
- **Questions:** Ask in #ai-testing Slack channel
- **Issues:** Create GitHub issue with `fine-tuning` label
- **Data requests:** What examples should v1.0 include?

---

## üìÖ Timeline

- **Today (2026-01-01):** Deploy v0.1 for testing
- **Week 1-2:** Internal testing, gather feedback
- **Week 2:** Analyze results, plan v1.0 improvements
- **Week 3:** Build Session 3 (Art phase expansion)
- **Week 4:** Deploy v1.0 with improvements
- **Month 2:** External beta testing (selected partners)
- **Month 3:** Production deployment (public-facing)

---

**Document Version:** 1.0
**Last Updated:** 2026-01-01
**Next Review:** After 2 weeks of v0.1 testing
