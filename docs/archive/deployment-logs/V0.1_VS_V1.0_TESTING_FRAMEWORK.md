# ðŸ†š v0.1 vs v1.0 A/B Testing Framework
**Created:** 2026-01-01
**Status:** Both models deployed and training

---

## ðŸ“Š Model Comparison

### v0.1 - Internal Testing Model
- **Job ID:** `ftjob-NdLSB8DN6aRtm71xXvh1K6X0`
- **File ID:** `file-T9Yn61rXwWz4CdPEByqNbv`
- **Model Name:** `gpt-4o-mini-2024-07-18:act-voice-v0-1`
- **Examples:** 90
- **Quality Score:** 88/100 (Good - Ready for fine-tuning)
- **Status:** Training
- **Purpose:** Baseline testing, gather early feedback

**Strengths:**
- Strong Listen phase (35.6%)
- Good Curiosity phase (26.7%)
- Solid Action phase (53.3%)
- Good LCAA language (84.4%)
- High community-centered framing (95.6%)

**Weaknesses:**
- Art phase under-represented (15.6%)
- Missing Art of Social Impact pillar
- Lower regenerative metaphors (27.8%)
- Only 83.3% pillar coverage

---

### v1.0 - Production Model
- **Job ID:** `ftjob-TYiyc0UhliJXkPEbglpQXcK3`
- **File ID:** `file-5wQMk78AKeVW3N7tVw98X5`
- **Model Name:** `gpt-4o-mini-2024-07-18:act-voice-v1-0`
- **Examples:** 120
- **Quality Score:** 96/100 (Excellent - Outstanding dataset quality)
- **Status:** Training
- **Purpose:** Production deployment, maximum quality

**Strengths:**
- Balanced LCAA coverage (27.5% / 20.8% / 59.2% / 33.3%)
- 100% strategic pillar coverage
- Enhanced regenerative metaphors (45.8%)
- Comprehensive Art phase (+113% vs v0.1)
- Higher voice consistency (76.6%)

**Improvements over v0.1:**
- +30 examples (+33%)
- +8 quality points
- +17.7% Art phase coverage
- +16.7% pillar coverage
- +18% regenerative metaphor usage

---

## ðŸ§ª Testing Categories

### 1. LCAA Methodology Questions

Test model understanding of ACT's core methodology.

**Test Queries:**

```
Q1: What does Listen mean in ACT's LCAA methodology?
Expected: Deep listening to Country, Traditional Owners, community voices, land, challenges

Q2: How does Curiosity show up in ACT's work?
Expected: Co-design, prototyping, testing assumptions, iteration, emergent solutions

Q3: Explain the Action phase in LCAA.
Expected: Implementation with accountability, power transfer, profit-sharing, systemic change

Q4: What role does Art play in ACT's methodology?
Expected: Revolution begins with imagination, consciousness-shifting, truth-telling, making invisible visible
```

**Evaluation Criteria:**
- âœ… Accurate LCAA definitions
- âœ… Connects to real ACT examples
- âœ… Uses regenerative language
- âœ… Community-centered framing
- âœ… Avoids jargon

**Expected Difference:**
- v0.1: Strong on Listen, Curiosity, Action; weaker on Art
- v1.0: Strong across all four phases with Art examples

---

### 2. Project-Specific Knowledge

Test accuracy on ACT projects and programs.

**Test Queries:**

```
Q5: How did Curiosity shape the Empathy Ledger platform?
Expected: Questions about currency, prototypes tested, community feedback, iteration process

Q6: What are the key features of BCV (Birrarung Conservation Village)?
Expected: Artist residencies, Country connection, regenerative practices, Traditional Owner partnership

Q7: Tell me about JusticeHub's approach to youth justice.
Expected: Community-led, diversion programs, holistic support, keeping young people connected

Q8: What is The Gold.Phone installation?
Expected: Art installation connecting strangers across difference, provocation, empathy building
```

**Evaluation Criteria:**
- âœ… Factual accuracy
- âœ… Specific project details
- âœ… LCAA integration
- âœ… Strategic pillar connection
- âœ… Voice consistency

**Expected Difference:**
- v0.1: Good project knowledge, less Art installation detail
- v1.0: Comprehensive project knowledge including Art installations

---

### 3. Art & Creative Systems Change

Test understanding of Art as revolution and creative methodology.

**Test Queries:**

```
Q9: Why does ACT say "art is the first form of revolution"?
Expected: Changes consciousness before systems, imagination as practical tool, truth-telling

Q10: How does art integrate with Listen, Curiosity, and Action?
Expected: Art throughout all phases, not decoration, methodology vs. outcome

Q11: What's the difference between art as methodology vs. art as outcome in ACT's work?
Expected: Process vs. product, consciousness-shifting vs. deliverable, woven throughout

Q12: How does ACT measure the impact of art in systems change?
Expected: Cultural shift indicators, consciousness change, movement building, narrative power
```

**Evaluation Criteria:**
- âœ… Art philosophy understanding
- âœ… Connects Art to systems change
- âœ… Specific installation examples
- âœ… Art + LCAA integration
- âœ… Regenerative metaphors

**Expected Difference:**
- v0.1: Basic Art understanding, limited examples
- v1.0: Deep Art philosophy, comprehensive examples, strong metaphors

---

### 4. Voice & Tone Match

Test alignment with ACT's regenerative voice.

**Test Queries:**

```
Q13: Tell me about ACT's approach to community partnerships.
Expected: Power transfer, 40% profit-sharing, community-led, long-term relationships

Q14: How does ACT work with Traditional Owners and custodians?
Expected: Cultural protocols, sovereignty respect, listening to Country, Traditional Owner leadership

Q15: What makes ACT different from traditional consultancies?
Expected: Not extractive, not parachute, community-centered, LCAA methodology, systemic change

Q16: Describe ACT's vision for regenerative innovation.
Expected: Seeds/soil/cultivation metaphors, ecosystem thinking, long-term transformation
```

**Evaluation Criteria:**
- âœ… Regenerative metaphors (seeds, soil, growth, harvest)
- âœ… Community-centered framing
- âœ… Cultural protocol respect
- âœ… Anti-jargon language
- âœ… Honest about complexity

**Expected Difference:**
- v0.1: 27.8% regenerative metaphors, good community framing
- v1.0: 45.8% regenerative metaphors (+65%), enhanced voice throughout

---

### 5. Strategic Pillar Integration

Test understanding of ACT's six strategic pillars.

**Test Queries:**

```
Q17: What is the "Ethical Storytelling" strategic pillar?
Expected: Community narratives, truth-telling, making invisible visible, storytelling as methodology

Q18: How does "Justice Reimagined" show up in ACT's work?
Expected: JusticeHub, restorative practices, community-led solutions, systemic transformation

Q19: Explain "Circular Economy" as ACT practices it.
Expected: Regenerative systems, Goods project, value cycles, waste as resource

Q20: What does "Art of Social Impact" mean to ACT?
Expected: Art as revolution, consciousness-shifting, creative systems change, installations
```

**Evaluation Criteria:**
- âœ… All six pillars covered
- âœ… Connects to real projects
- âœ… LCAA integration
- âœ… Systemic change framing
- âœ… Specific examples

**Expected Difference:**
- v0.1: 83.3% pillar coverage (missing Art of Social Impact)
- v1.0: 100% pillar coverage (all six included)

---

## ðŸ“‹ Testing Protocol

### Step 1: Initial Comparison (When Both Complete)

1. **Wait for training completion** (~10-30 min each)
2. **Verify both models are ready**
   - Check: https://platform.openai.com/finetune
   - Confirm: Status = "succeeded"
3. **Test with same 20 queries** (listed above)
4. **Document responses** in comparison table

### Step 2: Response Evaluation

For each query, rate 1-5 on:

**Accuracy (1-5):**
- 5 = Completely accurate, specific details
- 4 = Mostly accurate, minor gaps
- 3 = Generally accurate, some errors
- 2 = Partially accurate, significant errors
- 1 = Inaccurate or hallucinated

**Voice Match (1-5):**
- 5 = Perfect ACT voice, regenerative metaphors, community-centered
- 4 = Strong voice, good metaphors, mostly centered
- 3 = Adequate voice, some metaphors, somewhat centered
- 2 = Weak voice, few metaphors, organization-centered
- 1 = Wrong voice, jargon, extractive framing

**LCAA Application (1-5):**
- 5 = Integrates all relevant LCAA phases naturally
- 4 = Integrates multiple phases well
- 3 = Mentions LCAA, basic integration
- 2 = Limited LCAA reference
- 1 = No LCAA integration

**Overall Quality (1-5):**
- 5 = Excellent, would use publicly
- 4 = Good, minor edits needed
- 3 = Adequate, moderate edits needed
- 2 = Poor, major edits needed
- 1 = Unusable

### Step 3: Aggregate Scoring

Calculate averages for each model:

```
Model Performance = (Accuracy + Voice + LCAA + Overall) / 4
```

**Success Thresholds:**
- 4.0+ = Production-ready
- 3.5-3.9 = Good, needs refinement
- 3.0-3.4 = Adequate, needs work
- < 3.0 = Not ready

### Step 4: Qualitative Analysis

Document:
- **Surprising strengths** - What worked better than expected?
- **Unexpected gaps** - What's still missing?
- **Voice consistency** - Does it sound like ACT?
- **Hallucinations** - Any invented facts?
- **User preference** - Which would you choose for each query?

---

## ðŸ“Š Comparison Template

```markdown
### Query: [Question]

**v0.1 Response:**
[Paste response]

**v1.0 Response:**
[Paste response]

**Scores:**
| Metric | v0.1 | v1.0 | Winner |
|--------|------|------|--------|
| Accuracy | X/5 | X/5 | v0.1/v1.0/tie |
| Voice Match | X/5 | X/5 | v0.1/v1.0/tie |
| LCAA Application | X/5 | X/5 | v0.1/v1.0/tie |
| Overall Quality | X/5 | X/5 | v0.1/v1.0/tie |

**Notes:**
[Key differences, strengths, weaknesses]
```

---

## ðŸŽ¯ Decision Framework

### When to Use v0.1
- Budget-constrained applications
- Quick testing scenarios
- Art-light use cases
- Internal-only applications

### When to Use v1.0
- Public-facing applications
- Art-heavy contexts (installations, creative work)
- Maximum quality requirements
- Complete LCAA coverage needed
- Full strategic pillar representation

### When to Iterate to v2.0
If testing reveals:
- Voice consistency < 4.0
- Accuracy issues in key areas
- Missing project knowledge
- Hallucinations or errors
- Team feedback indicates gaps

---

## ðŸ“ Testing Output Files

Create these files during testing:

1. **`testing-results/v0.1-responses.md`**
   - All v0.1 responses to 20 queries
   - Timestamps and model info

2. **`testing-results/v1.0-responses.md`**
   - All v1.0 responses to 20 queries
   - Timestamps and model info

3. **`testing-results/comparison-scores.csv`**
   - Quantitative scores for analysis
   - Columns: Query, Category, v0.1_Accuracy, v0.1_Voice, v0.1_LCAA, v0.1_Overall, v1.0_Accuracy, v1.0_Voice, v1.0_LCAA, v1.0_Overall, Winner

4. **`testing-results/qualitative-analysis.md`**
   - Surprising findings
   - Preference notes
   - Gaps identified
   - Recommendations for v2.0

5. **`testing-results/FINAL_RECOMMENDATION.md`**
   - Which model to use for production
   - When to use each model
   - Next iteration priorities

---

## â±ï¸ Testing Timeline

### Week 1: Initial Testing
- Day 1: Both models complete training
- Day 2: Run all 20 test queries
- Day 3: Score and analyze responses
- Day 4: Share with team for additional feedback

### Week 2: Team Feedback
- Day 5-7: Team members test with their own queries
- Day 8-9: Gather systematic feedback
- Day 10: Compile feedback into analysis

### Week 3: Decision
- Day 11-12: Final comparison analysis
- Day 13: Choose production model
- Day 14: Document v2.0 priorities if needed

---

## ðŸš€ Test Execution Code

### Python Testing Script

```python
from openai import OpenAI
import json
from datetime import datetime

client = OpenAI(api_key="your-key")

test_queries = [
    "What does Listen mean in ACT's LCAA methodology?",
    "How does Curiosity show up in ACT's work?",
    # ... all 20 queries
]

def test_model(model_name, queries):
    results = []
    for i, query in enumerate(queries, 1):
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": query}],
            temperature=0.7
        )
        results.append({
            "query_number": i,
            "query": query,
            "response": response.choices[0].message.content,
            "timestamp": datetime.now().isoformat()
        })
    return results

# Test both models
v0_1_results = test_model("gpt-4o-mini-2024-07-18:act-voice-v0-1", test_queries)
v1_0_results = test_model("gpt-4o-mini-2024-07-18:act-voice-v1-0", test_queries)

# Save results
with open("testing-results/v0.1-responses.json", "w") as f:
    json.dump(v0_1_results, f, indent=2)

with open("testing-results/v1.0-responses.json", "w") as f:
    json.dump(v1_0_results, f, indent=2)
```

---

## ðŸ“Š Expected Outcomes

### Predicted Performance

**Overall Scores:**
- v0.1: ~3.8/5.0 (Good, ready for testing)
- v1.0: ~4.3/5.0 (Excellent, ready for production)

**Category Performance:**

| Category | v0.1 | v1.0 | Advantage |
|----------|------|------|-----------|
| LCAA Methodology | 4.0 | 4.5 | v1.0 (Art phase) |
| Project Knowledge | 3.8 | 4.2 | v1.0 (completeness) |
| Art & Creativity | 3.2 | 4.6 | v1.0 (major gap in v0.1) |
| Voice & Tone | 3.9 | 4.4 | v1.0 (regenerative metaphors) |
| Strategic Pillars | 3.7 | 4.3 | v1.0 (100% coverage) |

**Predicted Winner:** v1.0 for production use

**v0.1 Value:**
- Validates training approach worked
- Provides baseline for comparison
- Useful for budget-constrained scenarios

---

## ðŸŽ“ Lessons to Document

Track these learnings for future iterations:

1. **Dataset Quality Impact**
   - Did +8 quality points (88â†’96) translate to better responses?
   - Was the quality score predictive of model performance?

2. **Example Count Impact**
   - Did +30 examples (90â†’120) matter significantly?
   - Diminishing returns point?

3. **LCAA Balance Impact**
   - Did balanced LCAA improve overall performance?
   - Was Art phase expansion worth it?

4. **Regenerative Metaphor Impact**
   - Did +18% metaphor usage improve voice match?
   - Threshold for natural vs. forced?

5. **Strategic Pillar Impact**
   - Did 100% coverage improve pillar responses?
   - Worth the additional examples?

---

## âœ… Success Criteria

### v0.1 Success = Internal Testing Validated
- âœ… Proves fine-tuning approach works
- âœ… Provides baseline comparison
- âœ… 70%+ accuracy on test queries
- âœ… Identifies gaps for v1.0

### v1.0 Success = Production-Ready
- âœ… 4.0+ average score across all categories
- âœ… 90%+ factual accuracy
- âœ… Strong voice match (regenerative, community-centered)
- âœ… Complete LCAA and pillar coverage
- âœ… Team confidence for public use

### Testing Success = Clear Decision
- âœ… Quantitative data supports model choice
- âœ… Qualitative feedback aligned
- âœ… Clear use cases for each model
- âœ… v2.0 priorities identified

---

**Status:** Framework complete, ready to execute when models finish training

**Models Training:**
- v0.1: ftjob-NdLSB8DN6aRtm71xXvh1K6X0
- v1.0: ftjob-TYiyc0UhliJXkPEbglpQXcK3

**Monitor:** https://platform.openai.com/finetune

**Next Action:** Wait for training completion (~10-30 min), then begin testing
